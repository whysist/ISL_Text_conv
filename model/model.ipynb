{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 35/35 [15:25<00:00, 26.45s/it]\n",
      "Processing test: 100%|██████████| 35/35 [01:56<00:00,  3.34s/it]\n",
      "Processing val: 100%|██████████| 35/35 [03:49<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Annotation complete! YOLO labels saved in 'processed_dataset/labels/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
    "def convert_to_yolo_format(img_width, img_height, x_min, y_min, x_max, y_max):\n",
    "    x_center = (x_min + x_max) / 2 / img_width\n",
    "    y_center = (y_min + y_max) / 2 / img_height\n",
    "    width = (x_max - x_min) / img_width\n",
    "    height = (y_max - y_min) / img_height\n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "dataset_path = r\"C:/Users/fortn/OneDrive/Desktop/ISL_Text_conv/processed_dataset\"\n",
    "output_label_path = os.path.join(dataset_path, \"labels\")\n",
    "for subset in [\"train\", \"test\", \"val\"]:\n",
    "    subset_path = os.path.join(dataset_path, subset)\n",
    "    label_output_path = os.path.join(output_label_path, subset)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(label_output_path, exist_ok=True)\n",
    "\n",
    "    # Get only class folders (0-9, A-Z)\n",
    "    class_folders = sorted([f for f in os.listdir(subset_path) if os.path.isdir(os.path.join(subset_path, f))])\n",
    "\n",
    "    for class_name in tqdm(class_folders, desc=f\"Processing {subset}\"):\n",
    "        class_folder = os.path.join(subset_path, class_name)\n",
    "        label_class_path = os.path.join(label_output_path, class_name)\n",
    "        os.makedirs(label_class_path, exist_ok=True)\n",
    "\n",
    "        # Process all images in the class folder\n",
    "        for img_name in os.listdir(class_folder):\n",
    "            if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):  # Skip non-image files\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(class_folder, img_name)\n",
    "            label_file_path = os.path.join(label_class_path, img_name.rsplit(\".\", 1)[0] + \".txt\")\n",
    "\n",
    "            # Read image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                continue  # Skip unreadable images\n",
    "\n",
    "            img_height, img_width, _ = image.shape\n",
    "\n",
    "            # Convert to RGB and process with MediaPipe\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(image_rgb)\n",
    "\n",
    "            # If hand detected, get bounding box and write annotation\n",
    "            if results.multi_hand_landmarks:\n",
    "                x_min, y_min, x_max, y_max = img_width, img_height, 0, 0  # Reset bounding box\n",
    "\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    for lm in hand_landmarks.landmark:\n",
    "                        x, y = int(lm.x * img_width), int(lm.y * img_height)\n",
    "                        x_min, y_min, x_max, y_max = min(x_min, x), min(y_min, y), max(x_max, x), max(y_max, y)\n",
    "\n",
    "                # Ensure valid bounding box (skip small detections)\n",
    "                if x_max > x_min and y_max > y_min:\n",
    "                    x_center, y_center, width, height = convert_to_yolo_format(img_width, img_height, x_min, y_min, x_max, y_max)\n",
    "\n",
    "                    # Save annotation file\n",
    "                    with open(label_file_path, \"w\") as f:\n",
    "                        f.write(f\"{class_name} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "print(\"✅ Annotation complete! YOLO labels saved in 'processed_dataset/labels/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Define dataset paths\n",
    "dataset_path = r\"C:/Users/fortn/OneDrive/Desktop/ISL_Text_conv/processed_dataset\"\n",
    "labels_path = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# Choose which subset to visualize: \"train\", \"test\", or \"val\"\n",
    "subset = \"train\"  # Change to \"test\" or \"val\" if needed\n",
    "\n",
    "# Set paths\n",
    "image_folder = os.path.join(dataset_path, subset)\n",
    "label_folder = os.path.join(labels_path, subset)\n",
    "\n",
    "# Get class folders (0-9, A-Z)\n",
    "class_folders = sorted([f for f in os.listdir(image_folder) if os.path.isdir(os.path.join(image_folder, f))])\n",
    "\n",
    "# Select a **random class folder**\n",
    "selected_class = random.choice(class_folders)\n",
    "class_image_folder = os.path.join(image_folder, selected_class)\n",
    "class_label_folder = os.path.join(label_folder, selected_class)\n",
    "\n",
    "# Select a **random image** from this class\n",
    "image_files = [f for f in os.listdir(class_image_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "if not image_files:\n",
    "    print(f\"No images found in {class_image_folder}\")\n",
    "    exit()\n",
    "\n",
    "selected_image = random.choice(image_files)\n",
    "image_path = os.path.join(class_image_folder, selected_image)\n",
    "label_path = os.path.join(class_label_folder, selected_image.rsplit(\".\", 1)[0] + \".txt\")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(image_path)\n",
    "if image is None:\n",
    "    print(f\"Failed to load image: {image_path}\")\n",
    "    exit()\n",
    "\n",
    "img_height, img_width, _ = image.shape\n",
    "\n",
    "# Read YOLO annotations\n",
    "if os.path.exists(label_path):\n",
    "    with open(label_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # Parse YOLO annotation (class_id x_center y_center width height)\n",
    "        values = line.strip().split()\n",
    "        class_id = values[0]\n",
    "        x_center, y_center, width, height = map(float, values[1:])\n",
    "\n",
    "        # Convert normalized YOLO values back to pixel coordinates\n",
    "        x1 = int((x_center - width / 2) * img_width)\n",
    "        y1 = int((y_center - height / 2) * img_height)\n",
    "        x2 = int((x_center + width / 2) * img_width)\n",
    "        y2 = int((y_center + height / 2) * img_height)\n",
    "\n",
    "        # Draw bounding box on the image\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"Class {class_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# Show the image with bounding box\n",
    "cv2.imshow(\"YOLO Annotated Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
